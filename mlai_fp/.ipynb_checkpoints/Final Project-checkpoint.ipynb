{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a52108",
   "metadata": {},
   "source": [
    "# ML & AL 2023 Spring Course Project\n",
    "### Jiang Yuchen 12232418\n",
    "\n",
    "In this project, we are asked to finish several tasks which aim to find the shortest path in a weighted directed graph, including dynamic programming, reinforcement learning(Q-learning) and value function approximation. We also try to train a model based on value function approximation such that the obtained solution can work even if you offer a different graph. All codes are included below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d14e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## necessary python packages\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48f4551",
   "metadata": {},
   "source": [
    "## Task 1  Weighted directed graph\n",
    "Write a python code to create a weighted directed graph with arbitrary number of\n",
    "nodes and edges, for which the edges number can be modified by some parameters\n",
    "\n",
    "## Task 2  Dynamic programming\n",
    "By using dynamic programming, solve the problem of finding a shortest path fromanygiven starting node to any given target node in the weighted directed graph. In other words, you need to find out the optimal policy and optimal trajectory. The solution should still work without re-do dynamic programming when changing the starting node and target node.\n",
    "\n",
    "We apply dynamic programming in WDGraph class, which is shortest_path_solution() method. The implementation uses the floyd_warshall algorithm to compute the shortest distances and keeps track of the previous nodes to reconstruct the path, in order to avoid re-doing dynamic programming when changing start and end node pairs. Here we construct the example to test the dynamic programming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6284561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WDGraph():\n",
    "    \"\"\"\n",
    "    We define WDGraph as a class for weighted directed graph, which is used through the subsequent tasks.\n",
    "    The class can init an instance of graph, add and delete edges.\n",
    "    Also, the class can find the shortest path for any pair of start and end nodes as it is initialized. \n",
    "    \"\"\"\n",
    "    def __init__(self, nodes, edges):\n",
    "        \"\"\"\n",
    "        self.graph is a dictionary to collect nodes with their connected nodes and weights\n",
    "        e.g. {node:[(cost,neighbour), ...]}\n",
    "        self.nodes and self.edges are lists to record all nodes and edges information.\n",
    "        e.g. [node_1, node_2,...], [(from_node, to_node, weight), ...]\n",
    "        self.init_graph() is called to construct the graph based on received nodes and edges\n",
    "        add_edge() and delete_edge() is designed to add/delete edge in the graph\n",
    "        shortest_path_fw() is designed to record global information of shortest path\n",
    "        \"\"\"\n",
    "        self.graph = dict()\n",
    "        self.nodes = nodes\n",
    "        self.edges = edges\n",
    "        self.node2num = {}    # easily record\n",
    "        self.init_graph(nodes, edges)\n",
    "        self.global_distance, self.global_path = self.shortest_path_fw()\n",
    "    \n",
    "    def init_graph(self, nodes, edges):\n",
    "        \"\"\"\n",
    "        constract the graph according to nodes and edges\n",
    "        \"\"\"\n",
    "        for i in range(len(nodes)):\n",
    "            self.graph[nodes[i]] = []\n",
    "            self.node2num[nodes[i]] = i\n",
    "        for edge in edges:\n",
    "            (from_node, to_node, weight) = edge\n",
    "            self.graph[from_node].append((weight, to_node))\n",
    "        \n",
    "    \n",
    "    def add_edge(self, from_node, to_node, weight):\n",
    "        \"\"\"\n",
    "        add an edge to the graph, input form is \"from_node, to_node and weight\"\n",
    "        if from_node or to_node not in node list, raise exception\n",
    "        \"\"\"\n",
    "        if from_node in self.nodes and to_node in self.nodes:\n",
    "            self.graph[from_node].append((weight, to_node))\n",
    "        else:\n",
    "            raise Exception(\"Added edge contains invalid node!\")\n",
    "\n",
    "    def delete_edge(self, from_node, to_node):\n",
    "        \"\"\"\n",
    "        delete an edge to the graph, input form is \"from_node, to_node\"\n",
    "        weight is not needed\n",
    "        if edge is invalid, raise exception\n",
    "        \"\"\"\n",
    "        if from_node not in self.nodes or to_node not in self.nodes:\n",
    "            raise Exception(\"Target edge contains invalid node!\")\n",
    "        for i in range(len(self.graph[from_node])):\n",
    "            if self.graph[from_node][i][1] == to_node:\n",
    "                self.graph[from_node].pop(i)\n",
    "                break\n",
    "                \n",
    "    def shortest_path_fw(self):\n",
    "        \"\"\"\n",
    "        find shortest path by floyd_warshall algorithm\n",
    "        You can run it by hand as edges are added or deleted to update global distance and path matrix\n",
    "        \"\"\"\n",
    "        # cannot search and give error if the graph is not constructed\n",
    "        if not self.graph:\n",
    "            raise Exception(\"Graph is not constructed!\")\n",
    "            \n",
    "        # Number of vertices in the graph\n",
    "        n = len(self.nodes)\n",
    "        \n",
    "        # Initialize the distance matrix with the graph's weights\n",
    "        # -1 for unreachble pair\n",
    "        dist = []\n",
    "        # Initialize path matrix\n",
    "        path = [[None] * n for _ in range(n)]\n",
    "        for i in range(n):\n",
    "            dist.append([])\n",
    "            for j in range(n):\n",
    "                if j == i:\n",
    "                    dist[-1].append(0)\n",
    "                else:\n",
    "                    dist[-1].append(-1)\n",
    "        for node in self.nodes:\n",
    "            for weight, neighbor in self.graph[node]:\n",
    "                node_id, neighbor_id = self.node2num[node], self.node2num[neighbor]\n",
    "                dist[node_id][neighbor_id] = weight\n",
    "                path[node_id][neighbor_id] = node_id\n",
    "\n",
    "        for i in range(len(dist)):\n",
    "            for j in range(len(dist[i])):\n",
    "                if dist[i][j] == -1:\n",
    "                    dist[i][j] = float(\"inf\")\n",
    "        \n",
    "        # Find the shortest path between all pairs of vertices\n",
    "        for k in range(n):\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    # If vertex k is on the shortest path from i to j, update the distance\n",
    "                    # record vertex k for i and j\n",
    "                    if dist[i][k] + dist[k][j] < dist[i][j]:  \n",
    "                        dist[i][j] = dist[i][k] + dist[k][j]\n",
    "                        path[i][j] = path[k][j]\n",
    "\n",
    "        return dist, path\n",
    "    \n",
    "    def get_shortest_path(self, start, end):\n",
    "        \"\"\"\n",
    "        reconstruct the shortest path according to self.global path and start, end nodes\n",
    "        \"\"\"\n",
    "        i, j = start, end\n",
    "        if self.global_path[i][j] is None:\n",
    "            return []\n",
    "\n",
    "        path = [j]\n",
    "        while i != j:\n",
    "            j = self.global_path[i][j]\n",
    "            if j is None:\n",
    "                break\n",
    "            path.append(j)\n",
    "\n",
    "        return path[::-1]\n",
    "    \n",
    "    def shortest_path_solution(self, start_node, end_node):\n",
    "        \"\"\"\n",
    "        give solution for shortest path recorded in self.dist and self.path by start and end node pair\n",
    "        \"\"\"\n",
    "        # If receive invalid node, raise exception\n",
    "        if start_node not in self.nodes or end_node not in self.nodes:\n",
    "            raise Exception(\"Invalid node!\")\n",
    "            \n",
    "        # turn node into id for find results\n",
    "        start_index = self.node2num[start_node]\n",
    "        end_index = self.node2num[end_node]\n",
    "        path = self.get_shortest_path(start_index, end_index)\n",
    "        \n",
    "        # change index into node for review\n",
    "        node_path = []\n",
    "        for idx in path:\n",
    "            node_path.append(self.nodes[idx])\n",
    "        return self.global_distance[start_index][end_index], node_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bb61ba",
   "metadata": {},
   "source": [
    "## Test 1\n",
    "Here we design an example to test task 1 and 2. The results show that the dynamic programming is successfully done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "470e76e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with given nodes and edges\n",
    "nodes = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]\n",
    "edges = [\n",
    "        (\"A\", \"B\", 7),\n",
    "        (\"A\", \"D\", 5),\n",
    "        (\"B\", \"C\", 8),\n",
    "        (\"B\", \"D\", 9),\n",
    "        (\"B\", \"E\", 7),\n",
    "        (\"C\", \"E\", 5),\n",
    "        (\"D\", \"E\", 15),\n",
    "        (\"D\", \"F\", 6),\n",
    "        (\"E\", \"F\", 8),\n",
    "        (\"E\", \"G\", 9),\n",
    "        (\"F\", \"G\", 11)\n",
    "    ]\n",
    "graph = WDGraph(nodes, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15daa245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0, 7, 15, 5, 14, 11, 22],\n",
       "  [inf, 0, 8, 9, 7, 15, 16],\n",
       "  [inf, inf, 0, inf, 5, 13, 14],\n",
       "  [inf, inf, inf, 0, 15, 6, 17],\n",
       "  [inf, inf, inf, inf, 0, 8, 9],\n",
       "  [inf, inf, inf, inf, inf, 0, 11],\n",
       "  [inf, inf, inf, inf, inf, inf, 0]],\n",
       " [[None, 0, 1, 0, 1, 3, 5],\n",
       "  [None, None, 1, 1, 1, 3, 4],\n",
       "  [None, None, None, None, 2, 4, 4],\n",
       "  [None, None, None, None, 3, 3, 5],\n",
       "  [None, None, None, None, None, 4, 4],\n",
       "  [None, None, None, None, None, None, 5],\n",
       "  [None, None, None, None, None, None, None]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.shortest_path_fw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f803ef8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': [(7, 'B'), (5, 'D')],\n",
       " 'B': [(8, 'C'), (9, 'D'), (7, 'E')],\n",
       " 'C': [(5, 'E')],\n",
       " 'D': [(15, 'E'), (6, 'F')],\n",
       " 'E': [(8, 'F'), (9, 'G')],\n",
       " 'F': [(11, 'G')],\n",
       " 'G': []}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review the constructed graph\n",
    "graph.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0d25e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, ['C', 'E', 'F'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give start and end node, find shortest path\n",
    "start_node, end_node = \"C\", \"F\"\n",
    "graph.shortest_path_solution(start_node, end_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e68e8b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, ['A', 'D', 'F', 'G'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can be used for multiple times\n",
    "start_node, end_node = \"A\", \"G\"\n",
    "graph.shortest_path_solution(start_node, end_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "890bad60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(inf, [])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return float(\"inf\") when unreachble\n",
    "start_node, end_node = \"C\", \"A\"\n",
    "graph.shortest_path_solution(start_node, end_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830cdd8b",
   "metadata": {},
   "source": [
    "## Task 3  Reinforcement learning (Q-learning)\n",
    "Solve the above problem by using one of these reinforcement learning techniques, such as MC, SARSA(0), SARSA($\\lambda$), Q-learning.\n",
    "\n",
    "## Task 4 Value function approximation\n",
    "Considering value function approximation, solve the above problem.\n",
    "\n",
    "Here we use Q-learning to solve the problem. The Q-values are iteratively updated based on the rewards obtained during training episodes. The `get_epsilon_greedy_action` function implements the epsilon-greedy policy for action selection.\n",
    "\n",
    "Besides, the value function approximation method is included in `RLShortestPath` class. The code demonstrates the usage of Value Function Approximation (VFA) to find the shortest path from a given starting node to a target node in the weighted directed graph. It uses a linear approximation function with weights to estimate the value of a state. The weights are updated through gradient descent to minimize the prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "461cd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLShortestPath:\n",
    "    def __init__(self, graph):\n",
    "        self.G = graph\n",
    "        self.R = {}    # record weight information for reward setting\n",
    "        self.q_values = self.init_q_values()    # apply q learning\n",
    "        self.weights = self.init_weights()     # apply value function approximation\n",
    "        self.node2num = {}    # easily record, used in VFA\n",
    "        for i in range(len(graph.nodes)):\n",
    "            self.node2num[graph.nodes[i]] = i\n",
    "        self.q_ans = self.q_learning()    # record global shortest path answer by q-learning\n",
    "        self.vfa_ans = self.VFA()    # record global shortest path answer by value function approximation\n",
    "        self.q_time_cost, self.vfa_time_cost = 0, 0    # record time cost, since main process happen in __init__\n",
    "    \n",
    "    def path2cost(self, path):\n",
    "        \"\"\"\n",
    "        count cost on given path generated from Q-learning and VFA\n",
    "        \"\"\"\n",
    "        cost = 0\n",
    "        curr_node = path[0]\n",
    "        for i in range(1, len(path)):\n",
    "            to_node = path[i]\n",
    "            for weight, neighbor in self.G.graph[curr_node]:\n",
    "                if neighbor == to_node:\n",
    "                    cost += weight\n",
    "                    break\n",
    "            curr_node = path[i]\n",
    "        return cost\n",
    "\n",
    "    \"\"\"\n",
    "    Q-learning Part:\n",
    "    1. init q values for each node\n",
    "    2. train q learning for start node and end node\n",
    "    3. update q value during training according to epsilon greedy action\n",
    "    4. find shortest path according to updated q values\n",
    "    \"\"\"\n",
    "    def init_q_values(self):\n",
    "        \"\"\"\n",
    "        all q values are set to 0 for reachable pair\n",
    "        record weight information by the way\n",
    "        \"\"\"\n",
    "        q_values = {}\n",
    "        for node in self.G.nodes:\n",
    "            q_values[node] = {}\n",
    "            for weight, neighbor in self.G.graph[node]:\n",
    "                q_values[node][neighbor] = 0\n",
    "                pair = node+neighbor\n",
    "                self.R[pair] = weight\n",
    "        return q_values\n",
    "\n",
    "    def train_q_learning(self, start_node, target_node, num_episodes=1500, learning_rate=0.2, discount_factor=0.9):\n",
    "        \"\"\"\n",
    "        main part for training Q-learning\n",
    "        set num_episodes, learning_rate and discount_factor for training\n",
    "        \"\"\"\n",
    "        # in order to avoid possible cycle in graph which leads to infinite loop, count duplicate visited node\n",
    "        # as duplicated visit time is larger than threshold, break the loop\n",
    "        cycle_threshold = 5\n",
    "        for episode in range(num_episodes):\n",
    "            current_node = start_node\n",
    "            cycle_detect = []\n",
    "            detect_times = 0\n",
    "            #  The Q-values are iteratively updated based on the rewards obtained during training episodes. \n",
    "            #  The get_epsilon_greedy_action function implements the epsilon-greedy policy for action selection.\n",
    "            while current_node != target_node:\n",
    "                if not self.q_values[current_node]:\n",
    "                    break\n",
    "                cycle_detect.append(current_node)\n",
    "                action = self.get_epsilon_greedy_action(current_node)\n",
    "                next_node = action\n",
    "                # set reward as negative of edge weight and 1-weight if next node is end node, which aims for shortest path\n",
    "                reward = 1-self.R[current_node+next_node] if next_node == target_node else -self.R[current_node+next_node]\n",
    "                self.update_q_value(current_node, action, next_node, reward, learning_rate, discount_factor)\n",
    "                current_node = next_node\n",
    "                # cycle detection\n",
    "                if current_node in cycle_detect:\n",
    "                    detect_times += 1\n",
    "                if detect_times > cycle_threshold:\n",
    "                    break\n",
    "\n",
    "    def get_epsilon_greedy_action(self, node, epsilon=0.2):\n",
    "        \"\"\"\n",
    "        Implements the epsilon-greedy policy for action selection.\n",
    "        \"\"\"\n",
    "        if random.random() < epsilon:\n",
    "            (weight, action) = random.choice(list(self.G.graph[node]))\n",
    "            return action\n",
    "        else:\n",
    "            return max(self.q_values[node], key=self.q_values[node].get)\n",
    "\n",
    "    def update_q_value(self, current_node, action, next_node, reward, learning_rate, discount_factor):\n",
    "        \"\"\"\n",
    "        update q values based on equation\n",
    "        \"\"\"\n",
    "        if not self.q_values[next_node]:\n",
    "            max_q_value = 0\n",
    "        else:\n",
    "            max_q_value = max(self.q_values[next_node].values())\n",
    "        self.q_values[current_node][action] += learning_rate * (reward + \n",
    "                    discount_factor * max_q_value - self.q_values[current_node][action])\n",
    "\n",
    "    def shortest_path_based_on_q_value(self, start_node, target_node):\n",
    "        \"\"\"\n",
    "        find shortest path based on updated q values\n",
    "        from start node, we choose each action with maximum q value\n",
    "        return infinite if unreachble\n",
    "        \"\"\"\n",
    "        cycle_threshold = 5\n",
    "        detect_times = 0\n",
    "        cycle_detect = []\n",
    "        current_node = start_node\n",
    "        path = [current_node]\n",
    "        while current_node != target_node:\n",
    "            if not self.q_values[current_node]:\n",
    "                return float(\"inf\")\n",
    "            cycle_detect.append(current_node)\n",
    "            action = max(self.q_values[current_node], key=self.q_values[current_node].get)\n",
    "            next_node = action\n",
    "            path.append(next_node)\n",
    "            current_node = next_node\n",
    "            if current_node in cycle_detect:\n",
    "                detect_times += 1\n",
    "            if detect_times > cycle_threshold:\n",
    "                return float(\"inf\")\n",
    "        return path\n",
    "    \n",
    "    def q_learning(self):\n",
    "        \"\"\"\n",
    "        The main part of Q-learning\n",
    "        The single pair of start and end node in Q-learning is described above\n",
    "        In order to maintain whole graph, run init_q_values and train_q_learning for all possible pair\n",
    "        \"\"\"\n",
    "        q_start_time = time.time()\n",
    "        ans = {}\n",
    "        for start in self.G.nodes:\n",
    "            for end in self.G.nodes:\n",
    "                self.q_values = self.init_q_values()\n",
    "                self.train_q_learning(start, end)\n",
    "                path = self.shortest_path_based_on_q_value(start, end)\n",
    "                pair = start + end\n",
    "                # if reachble, get the cost of the path\n",
    "                if path != float(\"inf\"):\n",
    "                    ans[pair] = (path, self.path2cost(path))\n",
    "                else:\n",
    "                    ans[pair] = ([], float(\"inf\"))\n",
    "        self.q_time_cost = time.time() - q_start_time\n",
    "        print(\"Time cost for Q-Learning:\", self.q_time_cost, \"s\")    # output q-learning time cost\n",
    "        return ans\n",
    "    \n",
    "    def q_learning_solution(self, start, end):\n",
    "        \"\"\"\n",
    "        Get solution from q-learning's answer\n",
    "        \"\"\"\n",
    "        return self.q_ans[start+end]\n",
    "        \n",
    "    \"\"\"\n",
    "    Value Function Approximation Part:\n",
    "    1. design features and init weights for value function\n",
    "    2. train value function by episodes\n",
    "    3. update weights during training\n",
    "    4. find shortest path by VFA\n",
    "    \"\"\"\n",
    "    def init_weights(self):\n",
    "        num_features = len(self.G.graph) * 2  # Features: node visited (0/1) for each node\n",
    "        weights = np.zeros(num_features)\n",
    "        return weights\n",
    "    \n",
    "    def get_features(self, node, visited_nodes):\n",
    "        num_nodes = len(self.G.graph)\n",
    "        features = np.zeros(num_nodes * 2)\n",
    "        node_id = self.node2num[node]\n",
    "        features[node_id] = 1\n",
    "        features[num_nodes + node_id] = visited_nodes.count(node)\n",
    "        return features\n",
    "    \n",
    "    def value_function(self, state):\n",
    "        return np.dot(self.weights, state)\n",
    "    \n",
    "    def update_weights(self, state, target, learning_rate):\n",
    "        prediction = self.value_function(state)\n",
    "        error = target - prediction\n",
    "        self.weights += learning_rate * error * state\n",
    "    \n",
    "    def train_value_function(self, start_node, target_node, num_episodes=1500, learning_rate=0.1, discount_factor=0.9):\n",
    "        # also detect possible cycle\n",
    "        cycle_threshold = 5\n",
    "        for episode in range(num_episodes):\n",
    "            current_node = start_node\n",
    "            visited_nodes = []\n",
    "            detect_times = 0\n",
    "            cycle_detect = []\n",
    "            while current_node != target_node:\n",
    "                visited_nodes.append(current_node)\n",
    "                state = self.get_features(current_node, visited_nodes)\n",
    "                if not self.G.graph[current_node]:\n",
    "                    break\n",
    "                cycle_detect.append(current_node)\n",
    "                next_node = random.choice([neighbor for _, neighbor in self.G.graph[current_node]])\n",
    "                reward = 1 if next_node == target_node else -1\n",
    "                next_state = self.get_features(next_node, visited_nodes)\n",
    "                target = reward + discount_factor * self.value_function(next_state)\n",
    "                self.update_weights(state, target, learning_rate)\n",
    "                current_node = next_node\n",
    "                # cycle detection\n",
    "                if current_node in cycle_detect:\n",
    "                    detect_times += 1\n",
    "                if detect_times > cycle_threshold:\n",
    "                    break\n",
    "    \n",
    "    def shortest_path_based_on_VFA(self, start_node, target_node):\n",
    "        cycle_threshold = 5\n",
    "        detect_times = 0\n",
    "        cycle_detect = []\n",
    "        current_node = start_node\n",
    "        path = [current_node]\n",
    "        while current_node != target_node:\n",
    "            visited_nodes = path\n",
    "            state = self.get_features(current_node, visited_nodes)\n",
    "            if not self.G.graph[current_node]:\n",
    "                return float(\"inf\")\n",
    "            cycle_detect.append(current_node)\n",
    "            next_node = random.choice([neighbor for _, neighbor in self.G.graph[current_node]])\n",
    "            path.append(next_node)\n",
    "            current_node = next_node\n",
    "            if current_node in cycle_detect:\n",
    "                detect_times += 1\n",
    "            if detect_times > cycle_threshold:\n",
    "                return float(\"inf\")\n",
    "        return path\n",
    "    \n",
    "    def VFA(self):\n",
    "        vfa_start_time = time.time()\n",
    "        ans = {}\n",
    "        for start in self.G.nodes:\n",
    "            for end in self.G.nodes:\n",
    "                self.weights = self.init_weights()\n",
    "                self.train_value_function(start, end)\n",
    "                path = self.shortest_path_based_on_VFA(start, end)\n",
    "                pair = start + end\n",
    "                if path != float(\"inf\"):\n",
    "                    ans[pair] = (path, self.path2cost(path))\n",
    "                else:\n",
    "                    ans[pair] = ([], float(\"inf\"))\n",
    "        self.vfa_time_cost = time.time() - vfa_start_time\n",
    "        print(\"Time cost for Value Function Approximation:\", self.vfa_time_cost, \"s\")\n",
    "        return ans\n",
    "    \n",
    "    def VFA_solution(self, start, end):\n",
    "        return self.vfa_ans[start+end]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8892c36d",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "Also, we design the given exmaple and an random example to compare dynamic programming, Q-learning and Value Function Approximation methods. Detail analysis is described in report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4220b0c0",
   "metadata": {},
   "source": [
    "### test with given nodes and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b561ba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]\n",
    "edges = [\n",
    "        (\"A\", \"B\", 7),\n",
    "        (\"A\", \"D\", 5),\n",
    "        (\"B\", \"C\", 8),\n",
    "        (\"B\", \"D\", 9),\n",
    "        (\"B\", \"E\", 7),\n",
    "        (\"C\", \"E\", 5),\n",
    "        (\"D\", \"E\", 15),\n",
    "        (\"D\", \"F\", 6),\n",
    "        (\"E\", \"F\", 8),\n",
    "        (\"E\", \"G\", 9),\n",
    "        (\"F\", \"G\", 11)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b16ba214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 0.0004220008850097656 s\n"
     ]
    }
   ],
   "source": [
    "# Dynamic Programming\n",
    "start_time = time.time()\n",
    "graph = WDGraph(nodes, edges)\n",
    "time_cost = time.time() - start_time\n",
    "print(\"Time cost:\", time_cost, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c56cbae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost for Q-Learning: 0.19626808166503906 s\n",
      "Time cost for Value Function Approximation: 1.2574656009674072 s\n",
      "Time cost: 1.4540777206420898 s\n"
     ]
    }
   ],
   "source": [
    "# Q-learning and VFA\n",
    "start_time = time.time()\n",
    "rl = RLShortestPath(graph)\n",
    "time_cost = time.time() - start_time\n",
    "print(\"Time cost:\", time_cost, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "21c5bac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': [(7, 'B'), (5, 'D')],\n",
       " 'B': [(8, 'C'), (9, 'D'), (7, 'E')],\n",
       " 'C': [(5, 'E')],\n",
       " 'D': [(15, 'E'), (6, 'F')],\n",
       " 'E': [(8, 'F'), (9, 'G')],\n",
       " 'F': [(11, 'G')],\n",
       " 'G': []}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5ae4ee51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, ['A', 'D', 'F'])\n",
      "(['A', 'D', 'F'], 11)\n",
      "(['A', 'D', 'F'], 11)\n"
     ]
    }
   ],
   "source": [
    "s, e = \"A\", \"F\"\n",
    "print(graph.shortest_path_solution(s, e))\n",
    "print(rl.q_learning_solution(s, e))\n",
    "print(rl.VFA_solution(s, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7ef77051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(inf, [])\n",
      "([], inf)\n",
      "([], inf)\n"
     ]
    }
   ],
   "source": [
    "s, e = \"G\", \"A\"\n",
    "print(graph.shortest_path_solution(s, e))\n",
    "print(rl.q_learning_solution(s, e))\n",
    "print(rl.VFA_solution(s, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf2cd537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly generate graph\n",
    "def generate_graph(num_nodes=20):\n",
    "    nodes = random.sample(range(1, num_nodes+1), num_nodes)\n",
    "\n",
    "    edges_num = random.randint(num_nodes*2, num_nodes*5)    # set number of edges\n",
    "    edges_nodes = []\n",
    "    edges_weights = []\n",
    "    for _ in range(edges_num):\n",
    "        # randomly add edge which is not duplicate\n",
    "        from_node, to_node = random.sample(nodes, 2)\n",
    "        while (from_node, to_node) in edges_nodes:\n",
    "            from_node, to_node = random.sample(nodes, 2)\n",
    "        edges_nodes.append((from_node, to_node))\n",
    "        edges_weights.append(random.randint(1, 30))\n",
    "    edges = []\n",
    "    for i in range(edges_num):\n",
    "        edges.append((str(edges_nodes[i][0]), str(edges_nodes[i][1]), int(edges_weights[i])))\n",
    "    for i in range(len(nodes)):\n",
    "        nodes[i] = str(nodes[i])\n",
    "    return nodes, edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4388f366",
   "metadata": {},
   "source": [
    "Test with 10 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d43ea441",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes, edges = generate_graph(num_nodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "204bcecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 0.0009260177612304688 s\n"
     ]
    }
   ],
   "source": [
    "# Dynamic Programming\n",
    "start_time = time.time()\n",
    "graph = WDGraph(nodes, edges)\n",
    "time_cost = time.time() - start_time\n",
    "print(\"Time cost:\", time_cost, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e4968282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost for Q-Learning: 0.9140703678131104 s\n",
      "Time cost for Value Function Approximation: 10.919919729232788 s\n",
      "Time cost: 11.835331916809082 s\n"
     ]
    }
   ],
   "source": [
    "# Q-learning and VFA\n",
    "start_time = time.time()\n",
    "rl = RLShortestPath(graph)\n",
    "time_cost = time.time() - start_time\n",
    "print(\"Time cost:\", time_cost, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "85c828c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5': [(23, '2'), (17, '1'), (6, '7'), (17, '9'), (27, '10')],\n",
       " '7': [(28, '2'), (5, '9'), (21, '1')],\n",
       " '1': [(8, '6'), (7, '4'), (22, '8')],\n",
       " '10': [(6, '7'), (3, '8'), (26, '4')],\n",
       " '9': [(8, '4'), (28, '7'), (8, '1'), (21, '6')],\n",
       " '8': [(8, '9')],\n",
       " '3': [(26, '6'), (9, '4'), (1, '5')],\n",
       " '6': [(15, '10'), (19, '2')],\n",
       " '2': [(24, '4'), (26, '9')],\n",
       " '4': [(17, '1'), (16, '10'), (13, '3'), (30, '8'), (4, '7'), (15, '5')]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "052b4ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, ['1', '4', '7'])\n",
      "(['1', '4', '7'], 11)\n",
      "(['1', '4', '3', '4', '8', '9', '7'], 95)\n"
     ]
    }
   ],
   "source": [
    "s, e = \"1\", \"7\"\n",
    "print(graph.shortest_path_solution(s, e))\n",
    "print(rl.q_learning_solution(s, e))\n",
    "print(rl.VFA_solution(s, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74faa292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(inf, [])\n",
      "([], inf)\n",
      "([], inf)\n"
     ]
    }
   ],
   "source": [
    "s, e = \"1\", \"8\"\n",
    "print(graph.shortest_path_solution(s, e))\n",
    "print(rl.q_learning_solution(s, e))\n",
    "print(rl.VFA_solution(s, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c72b513",
   "metadata": {},
   "source": [
    "Test with 40 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2ba42c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost: 0.017249345779418945 s\n",
      "Time cost for Q-Learning: 33.68354105949402 s\n",
      "Time cost for Value Function Approximation: 465.76904702186584 s\n",
      "Time cost: 499.4538698196411 s\n"
     ]
    }
   ],
   "source": [
    "nodes, edges = generate_graph(num_nodes=40)\n",
    "\n",
    "# Dynamic Programming\n",
    "start_time = time.time()\n",
    "graph = WDGraph(nodes, edges)\n",
    "time_cost = time.time() - start_time\n",
    "print(\"Time cost:\", time_cost, \"s\")\n",
    "\n",
    "# Q-learning and VFA\n",
    "start_time = time.time()\n",
    "rl = RLShortestPath(graph)\n",
    "time_cost = time.time() - start_time\n",
    "print(\"Time cost:\", time_cost, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1029fc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39, ['2', '8', '34', '30', '32', '17', '7'])\n",
      "(['2', '8', '34', '30', '32', '17', '7'], 39)\n",
      "([], inf)\n"
     ]
    }
   ],
   "source": [
    "s, e = \"2\", \"7\"\n",
    "print(graph.shortest_path_solution(s, e))\n",
    "print(rl.q_learning_solution(s, e))\n",
    "print(rl.VFA_solution(s, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258fd2e",
   "metadata": {},
   "source": [
    "# Task 5 \n",
    "Train a model based on value function approximation such that the obtained solutioncan work even if you offer a different graph. In other words, the obtained solution canbeapplied to different weighted directed graphs without re-training.\n",
    "\n",
    "Here I just provide a possbile model but have no time for furthre training.\n",
    "\n",
    "The `ValueFunctionApproximator` class provide the VFA model for different weighted directed graphs without re-training. We simply design full-connected layers and use `ReLU` function as activation function. We need a dataset which contains enough weighted directed graphs for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "466ec082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuchen/anaconda3/envs/tte/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class ValueFunctionApproximator(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(ValueFunctionApproximator, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class ShortestPathVFA:\n",
    "    def __init__(self, graph):\n",
    "        self.graph = graph\n",
    "        self.model = None\n",
    "\n",
    "    def initialize_model(self, num_features):\n",
    "        self.model = ValueFunctionApproximator(num_features)\n",
    "\n",
    "    def get_features(self, node, visited_nodes):\n",
    "        num_nodes = len(self.graph)\n",
    "        features = np.zeros(num_nodes * 2)\n",
    "        features[node] = 1\n",
    "        features[num_nodes + node] = visited_nodes.count(node)\n",
    "        return features\n",
    "\n",
    "    def value_function(self, state):\n",
    "        state_tensor = torch.from_numpy(state).float()\n",
    "        return self.model(state_tensor).item()\n",
    "\n",
    "    def update_model(self, states, targets, learning_rate):\n",
    "        states_tensor = torch.from_numpy(states).float()\n",
    "        targets_tensor = torch.from_numpy(targets).float()\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.SGD(self.model.parameters(), lr=learning_rate)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = self.model(states_tensor)\n",
    "        loss = criterion(outputs, targets_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    def train_value_function(self, start_node, target_node, num_episodes=100, learning_rate=0.1, discount_factor=0.9):\n",
    "        num_nodes = len(self.graph)\n",
    "        num_features = num_nodes * 2\n",
    "        self.initialize_model(num_features)\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            current_node = start_node\n",
    "            visited_nodes = []\n",
    "            states = []\n",
    "            targets = []\n",
    "            while current_node != target_node:\n",
    "                visited_nodes.append(current_node)\n",
    "                state = self.get_features(current_node, visited_nodes)\n",
    "                next_node = self.get_next_node(current_node)\n",
    "                reward = self.get_reward(next_node, target_node)\n",
    "                next_state = self.get_features(next_node, visited_nodes)\n",
    "                target = reward + discount_factor * self.value_function(next_state)\n",
    "                states.append(state)\n",
    "                targets.append(target)\n",
    "                current_node = next_node\n",
    "\n",
    "            self.update_model(np.array(states), np.array(targets), learning_rate)\n",
    "\n",
    "    def get_next_node(self, current_node):\n",
    "        neighbors = [neighbor for neighbor, _ in self.graph[current_node]]\n",
    "        return np.random.choice(neighbors)\n",
    "\n",
    "    def get_reward(self, next_node, target_node):\n",
    "        return 1 if next_node == target_node else -1\n",
    "\n",
    "    def shortest_path(self, start_node, target_node):\n",
    "        current_node = start_node\n",
    "        path = [current_node]\n",
    "        while current_node != target_node:\n",
    "            visited_nodes = path\n",
    "            state = self.get_features(current_node, visited_nodes)\n",
    "            state_tensor = torch.from_numpy(state).float()\n",
    "            action_value = self.model(state_tensor).detach().numpy()\n",
    "            next_node = self.get_next_node(current_node)\n",
    "            path.append(next_node)\n",
    "            current_node = next_node\n",
    "        return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ca370f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tte",
   "language": "python",
   "name": "tte"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
